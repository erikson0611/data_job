{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to get the job data by querying 資料分析, 資料工程, 數據分析, 數據工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of the results 6001002000\n",
      "end of the results 6001003000\n",
      "end of the results 6001004000\n",
      "end of the results 6001005000\n",
      "end of the results 6001006000\n",
      "end of the results 6001007000\n",
      "end of the results 6001008000\n",
      "end of the results 6001009000\n",
      "end of the results 6001010000\n",
      "end of the results 6001011000\n",
      "end of the results 6001012000\n",
      "end of the results 6001013000\n",
      "end of the results 6001014000\n",
      "end of the results 6001015000\n",
      "end of the results 6001016000\n",
      "end of the results 6001017000\n",
      "end of the results 6001018000\n",
      "end of the results 6001019000\n",
      "end of the results 6001020000\n",
      "end of the results 6001001001\n",
      "end of the results 6001001002\n",
      "end of the results 6001001003\n",
      "end of the results 6001001004\n",
      "end of the results 6001001005\n",
      "end of the results 6001001006\n",
      "end of the results 6001001007\n",
      "end of the results 6001001008\n",
      "end of the results 6001001009\n",
      "end of the results 6001001010\n",
      "end of the results 6001001011\n",
      "end of the results 6001001012\n",
      "1508\n"
     ]
    }
   ],
   "source": [
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "keyword = '數據工程' # set keyword\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "id_list = [] # lisf for saving the job_id\n",
    "\n",
    "# due to the 100 pages limitation, we query by area\n",
    "area_list = ['6001002000','6001003000','6001004000','6001005000','6001006000',\n",
    "            '6001007000','6001008000','6001009000','6001010000','6001011000','6001012000',\n",
    "            '6001013000','6001014000','6001015000','6001016000','6001017000','6001018000',\n",
    "            '6001019000','6001020000',\n",
    "            '6001001001','6001001002','6001001003','6001001004','6001001005','6001001006',\n",
    "            '6001001007','6001001008','6001001009','6001001010','6001001011','6001001012',\n",
    "            ]\n",
    "for area in area_list:\n",
    "    for page in range(0,101): #use the pages to check all the pages\n",
    "        my_params = {'ro':'1', # full time\n",
    "                     'keyword':'{}'.format(keyword), # set the keywords\n",
    "                     'area':'{}'.format(area), # city is taipei\n",
    "                     'page' : '{}'.format(page),\n",
    "                     'mode':'l'} # set the listing browse mode\n",
    "\n",
    "        response = requests.get('https://www.104.com.tw/jobs/search/?' , my_params, headers = headers)\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        try: # use the keyword to check if we found the last page\n",
    "            re.search(r'\\u5443\\u62cd\\u8b1d', str(soup)).group()\n",
    "            print('end of the results {}'.format(area))\n",
    "            break\n",
    "        except:\n",
    "#             print('opening the page')\n",
    "            for item in soup.select('a.js-job-link'):\n",
    "                full_link = item.get('href')\n",
    "                link_id = re.search(r'job\\/(.{5})', full_link).group(1)\n",
    "                id_list.append([link_id])\n",
    "print(len(id_list))\n",
    "\n",
    "with open('job_id.csv', 'a', newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening the page\n",
      "opening the page\n",
      "opening the page\n",
      "end of the results\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "id_list = [] # lisf for saving the job_id\n",
    "list_area = ['6001001000','6001002000','6001003000','6001004000','6001005000','6001006000',\n",
    "            '6001007000','6001008000','6001009000','6001010000','6001011000','6001012000',\n",
    "            '6001013000','6001014000','6001015000','6001016000','6001017000','6001018000',\n",
    "            '6001019000','6001020000']\n",
    "\n",
    "for page in range(0,100): #use the pages to check all the pages\n",
    "    my_params = {'ro':'1', # full time\n",
    "                 'keyword':'台灣大學', # set the keywords\n",
    "                 'area':'6001001000', # city is taipei\n",
    "    #              'isnew':'30', # update in last one month\n",
    "                 'page' : '{}'.format(page),\n",
    "                 'mode':'l'} # set the listing browse mode\n",
    "\n",
    "    response = requests.get('https://www.104.com.tw/jobs/search/?' , my_params, headers = headers)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    try: # use the keyword to check if we found the last page\n",
    "        re.search(r'\\u5443\\u62cd\\u8b1d', str(soup)).group()\n",
    "        print('end of the results')\n",
    "        break\n",
    "    except:\n",
    "        print('opening the page')\n",
    "        for item in soup.select('a.js-job-link'):\n",
    "            full_link = item.get('href')\n",
    "            link_id = re.search(r'job\\/(.{5})', full_link).group(1)\n",
    "            id_list.append(link_id)\n",
    "print(len(id_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9710\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate\n",
    "import pandas as pd\n",
    "df = pd.read_csv('job_id.csv')\n",
    "df = df.drop_duplicates()\n",
    "job_id_list = df.values.tolist()\n",
    "print(len(job_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, time, requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "\n",
    "my_params = {'ro':'1', # 限定全職的工作，如果不限定則輸入0\n",
    "             'keyword':'工程', # 想要查詢的關鍵字\n",
    "             'area':'6001001000', # 限定在台北的工作\n",
    "             'isnew':'30', # 只要最近一個月有更新的過的職缺\n",
    "             'mode':'l'} # 清單的瀏覽模式\n",
    "\n",
    "url = requests.get('https://www.104.com.tw/jobs/search/?' , my_params, headers = headers).url\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "for i in range(20): \n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    time.sleep(1)\n",
    "    \n",
    "k = 1\n",
    "while k != 0:\n",
    "    try:\n",
    "        driver.find_elements_by_class_name(\"js-more-page\").click() \n",
    "#         driver.find_element_by_xpath(\"//*[contains(text(),'手動載入')]\").click()\n",
    "        print('Click 手動載入，' + '載入第' + str(15 + k) + '頁')\n",
    "        k += 1\n",
    "        time.sleep(1) # 時間設定太短的話，來不及載入新資料就會跳錯誤\n",
    "    except:\n",
    "        k = 0\n",
    "        print('No more Job')\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "List = soup.findAll('a',{'class':'js-job-link'})\n",
    "print('共有 ' + str(len(List)) + ' 筆資料')\n",
    "\n",
    "print(len(List))\n",
    "\n",
    "def bind(cate):\n",
    "    k = []\n",
    "    for i in cate:\n",
    "        if len(i.text) > 0:\n",
    "            k.append(i.text)\n",
    "    return str(k)\n",
    "\n",
    "\n",
    "# JobList = pd.DataFrame()\n",
    "\n",
    "# i = 0\n",
    "# while i < len(List):\n",
    "#     print('正在處理第' + str(i) + '筆，共 ' + str(len(List)) + ' 筆資料')\n",
    "#     content = List[i]\n",
    "#     # 這裡用Try的原因是，有時候爬太快會遭到系統阻擋導致失敗。因此透過這個方式，當我們遇到錯誤時，會重新再爬一次資料！\n",
    "# #     try:\n",
    "#     print('https://' + content.attrs['href'].strip('//'))\n",
    "#     resp = requests.get('https://' + content.attrs['href'].strip('//'))\n",
    "#     print('準備BeautifulSoup')\n",
    "#     soup2 = BeautifulSoup(resp.text,'html.parser')\n",
    "#     print(soup2)\n",
    "#     df = pd.DataFrame(\n",
    "#         data = [{\n",
    "#             '公司名稱':soup2.find('a', {'class':'cn'}).text,\n",
    "#             '工作職稱':content.attrs['title'],\n",
    "#             '工作內容':soup2.find('p').text,\n",
    "#             '職務類別':bind(soup2.findAll('dd', {'class':'cate'})[0].findAll('span')),\n",
    "#             '工作待遇':soup2.find('dd', {'class':'salary'}).text.split('\\n\\n',2)[0].replace(' ',''),\n",
    "#             '工作性質':soup2.select('div > dl > dd')[2].text,\n",
    "#             '上班地點':soup2.select('div > dl > dd')[3].text.split('\\n\\n',2)[0].split('\\n',2)[1].replace(' ',''),\n",
    "#             '管理責任':soup2.select('div > dl > dd')[4].text,\n",
    "#             '出差外派':soup2.select('div > dl > dd')[5].text,\n",
    "#             '上班時段':soup2.select('div > dl > dd')[6].text,\n",
    "#             '休假制度':soup2.select('div > dl > dd')[7].text,\n",
    "#             '可上班日':soup2.select('div > dl > dd')[8].text,\n",
    "#             '需求人數':soup2.select('div > dl > dd')[9].text,\n",
    "#             '接受身份':soup2.select('div.content > dl > dd')[10].text,\n",
    "#             '學歷要求':soup2.select('div.content > dl > dd')[12].text,\n",
    "#             '工作經歷':soup2.select('div.content > dl > dd')[11].text,\n",
    "#             '語文條件':soup2.select('div.content > dl > dd')[14].text,\n",
    "#             '擅長工具':soup2.select('div.content > dl > dd')[15].text,\n",
    "#             '工作技能':soup2.select('div.content > dl > dd')[16].text,\n",
    "#             '其他條件':soup2.select('div.content > dl > dd')[17].text,\n",
    "#             '公司福利':soup2.select('div.content > p')[1].text,\n",
    "#             '科系要求':soup2.select('div.content > dl > dd')[13].text,\n",
    "#             '聯絡方式':soup2.select('div.content')[3].text.replace('\\n',''),\n",
    "#             '連結路徑':'https://' + content.attrs['href'].strip('//')}],\n",
    "#         columns = ['公司名稱','工作職稱','工作內容','職務類別','工作待遇','工作性質','上班地點','管理責任','出差外派',\n",
    "#                    '上班時段','休假制度','可上班日','需求人數','接受身份','學歷要求','工作經歷','語文條件','擅長工具',\n",
    "#                    '工作技能','其他條件','公司福利','科系要求','聯絡方式','連結路徑'])\n",
    "#     print('list加入')\n",
    "#     JobList = JobList.append(df, ignore_index=True)\n",
    "#     i += 1\n",
    "#     print(\"Success and Crawl Next 目前正在爬第\" + str(i) + \"個職缺資訊\")\n",
    "#     time.sleep(1) # 執行完休息0.5秒，避免造成對方主機負擔\n",
    "#     except:\n",
    "#         print('break')\n",
    "#         break\n",
    "\n",
    "# JobList\n",
    "\n",
    "# JobList.to_csv('JobList2.xlsx', encoding='cp950')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def ungzip(data):\n",
    "    try: data=gzip.decompress(data)\n",
    "    except: pass\n",
    "    return data\n",
    "\n",
    "result_list = [['jobName', 'custName', 'industry', 'workExp', 'edu', 'major',\n",
    "    'language', 'localLanguage', 'specialty', 'skill', 'certificate', 'other',\n",
    "    'jobDescription', 'salary', 'salaryMin', 'salaryMax', 'addressRegion',\n",
    "    'addressDetail', 'manageResp', 'businessTrip', 'workPeriod', 'vacationPolicy',\n",
    "    'longitude', 'latitude']]\n",
    "\n",
    "count = 1\n",
    "\n",
    "for job_id in job_id_list:\n",
    "    count += 1\n",
    "    job_id = job_id[0]\n",
    "    url = 'https://www.104.com.tw/job/ajax/content/{}'.format(job_id)\n",
    "    headers = {\n",
    "        'Host': 'www.104.com.tw',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Referer': 'https://www.104.com.tw/job/{}?jobsource=n104bank2'.format(job_id),\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5'\n",
    "        }\n",
    "    try:\n",
    "        req = urllib.request.Request(url,headers=headers)\n",
    "        result = urllib.request.urlopen(req)\n",
    "        result_str = ungzip(result.read()).decode()\n",
    "        job_data = json.loads(result_str)\n",
    "    \n",
    "        # header\n",
    "        jobName = job_data['data']['header']['jobName']\n",
    "        custName = job_data['data']['header']['custName']\n",
    "\n",
    "        # industry\n",
    "        industry = job_data['data']['industry']\n",
    "\n",
    "        # condition\n",
    "        workExp = job_data['data']['condition']['workExp']\n",
    "        edu = job_data['data']['condition']['edu']\n",
    "        major = job_data['data']['condition']['major']\n",
    "        language = job_data['data']['condition']['language']\n",
    "        localLanguage = job_data['data']['condition']['localLanguage']\n",
    "        specialty = []\n",
    "        for i in range(0, 10):\n",
    "            try: \n",
    "                specialty.append(job_data['data']['condition']['specialty'][i]['description'])\n",
    "            except:\n",
    "                break\n",
    "        skill = job_data['data']['condition']['skill']\n",
    "        certificate = job_data['data']['condition']['certificate']\n",
    "        other = job_data['data']['condition']['other']\n",
    "\n",
    "        #job detail\n",
    "        jobDescription = job_data['data']['jobDetail']['jobDescription']\n",
    "        salary = job_data['data']['jobDetail']['salary']\n",
    "        salaryMin = job_data['data']['jobDetail']['salaryMin']\n",
    "        salaryMax = job_data['data']['jobDetail']['salaryMax']\n",
    "        addressRegion = job_data['data']['jobDetail']['addressRegion']\n",
    "        addressDetail = job_data['data']['jobDetail']['addressDetail']\n",
    "        manageResp = job_data['data']['jobDetail']['manageResp']\n",
    "        businessTrip = job_data['data']['jobDetail']['businessTrip']\n",
    "        workPeriod = job_data['data']['jobDetail']['workPeriod']\n",
    "        vacationPolicy = job_data['data']['jobDetail']['vacationPolicy']\n",
    "        longitude = job_data['data']['jobDetail']['longitude']\n",
    "        latitude = job_data['data']['jobDetail']['latitude']\n",
    "\n",
    "        job_row = [jobName, custName, industry, workExp, edu, major, language, localLanguage,\n",
    "        specialty, skill, certificate, other, jobDescription, salary, salaryMin,\n",
    "        salaryMax, addressRegion, addressDetail, manageResp, businessTrip,\n",
    "        workPeriod, vacationPolicy, longitude, latitude]\n",
    "\n",
    "        result_list.append(job_row)\n",
    "        print(count)\n",
    "    except:\n",
    "        print(url)\n",
    "        continue\n",
    "    \n",
    "with open('job_rawdata.csv', 'a', newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salaryMin</th>\n",
       "      <th>salaryMax</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.699000e+03</td>\n",
       "      <td>9.699000e+03</td>\n",
       "      <td>9698.000000</td>\n",
       "      <td>9698.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.036044e+04</td>\n",
       "      <td>8.861038e+05</td>\n",
       "      <td>121.051329</td>\n",
       "      <td>24.597408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.033590e+05</td>\n",
       "      <td>2.777543e+06</td>\n",
       "      <td>4.286330</td>\n",
       "      <td>1.135837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>120.717015</td>\n",
       "      <td>24.246434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.800000e+04</td>\n",
       "      <td>3.500000e+04</td>\n",
       "      <td>121.485683</td>\n",
       "      <td>25.024944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.300000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>121.549783</td>\n",
       "      <td>25.054159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000e+06</td>\n",
       "      <td>9.999999e+06</td>\n",
       "      <td>121.853326</td>\n",
       "      <td>25.243263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          salaryMin     salaryMax    longitude     latitude\n",
       "count  9.699000e+03  9.699000e+03  9698.000000  9698.000000\n",
       "mean   3.036044e+04  8.861038e+05   121.051329    24.597408\n",
       "std    1.033590e+05  2.777543e+06     4.286330     1.135837\n",
       "min    0.000000e+00  0.000000e+00     0.000000     0.000000\n",
       "25%    0.000000e+00  0.000000e+00   120.717015    24.246434\n",
       "50%    2.800000e+04  3.500000e+04   121.485683    25.024944\n",
       "75%    3.300000e+04  5.000000e+04   121.549783    25.054159\n",
       "max    3.000000e+06  9.999999e+06   121.853326    25.243263"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df = pd.read_csv('job_rawdata.csv')\n",
    "job_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
